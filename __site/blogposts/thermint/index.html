<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/poole_hyde.css">
  <!-- style adjustments -->
  <style>
    html {font-size: 17px;}
    .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
    @media (min-width: 940px) {
      .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
    }
    @media (max-width: 768px) {
      .franklin-content {padding-left: 6%; padding-right: 6%;}
    }
  </style>
  <link rel="icon" href="/assets/favicon.png">
   <title>Thermodynamic Integration</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1><a href="/">Bayesian Tribulations</a></h1>
      <p class="lead">Unexpected computational events!</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item active" href="/blogposts/">Blog Posts</a>
      <a class="sidebar-nav-item " href="/publications/">Publications</a>
    </nav>
    <p>&copy; Theo Galy-Fajou.</p>
  </div>
</div>
<div class="content container">
<!-- Content appended here -->
<div class="franklin-content"><h1 id="thermodynamic_integration"><a href="#thermodynamic_integration">Thermodynamic Integration</a></h1>
<p><strong>Thermodynamic Integration &#40;TI&#41;</strong> is a method coming straight from statistical physics to compute the difference of free energy between two systems \(A\) and \(B\). We are going to go from this physics definition and see how we can apply it to Bayesian Inference.</p>
<p>We start with a system with a potential energy function \(U(x)\) with associated probability function</p>
\[p(x) = \frac{1}{Z}\exp\left(-\frac{U(x)}{k_BT}\right)\]
<p>Where \(Z\)  is the partition function of this system defined as</p>
\[Z = \int_\Omega \exp\left(-\frac{U(x)}{k_BT}\right)dx\]
<p>where \(\Omega\) is the domain of all the states of the system and \(x\) is one state of this system. The definition of the free energy is given by</p>
\[F = -k_BT\log(Z)\]
<p>Now going back to our systems \(A\) and \(B\) with potential energy \(U_A\) and \(U_B\) . We can construct a new system built as a linear interpolation between the two systems : \({U_\lambda = U_A + \lambda(U_B - U_A)}\)</p>
<p>We can show that</p>
\[F_B - F_A = \int_0^1 E_{U(\lambda)}\left[U_B - U_A\right]d\lambda\]
<p>&lt;details&gt;     &lt;summary&gt; Proof :&lt;/summary&gt;</p>
\[\int_0^1 \frac{\partial F(\lambda)}{\partial \lambda}d\lambda = \int_0^1 \frac{\partial \log(Z(\lambda)}{\partial \lambda}d\lambda = \]
<p>&lt;/details&gt;</p>
\[\begin{aligned}
F_B - F_A =& F(1) - F(0) = \int_0^1 \frac{\partial F(\lambda)}{\partial \lambda}d\lambda\\
\int_0^1 \frac{\partial F(\lambda)}{\partial \lambda}d\lambda &= -\int_0^1 \frac{\partial \log Z_\lambda}{\partial \lambda}d\lambda = -\int_0^1 \frac{1}{Z_\lambda}\frac{\partial Z_\lambda}{\partial \lambda}d\lambda \\
&=\int_0^1 \int_\Omega \frac{\exp(-U_\lambda(x))}{Z_\lambda}\frac{\partial U_\lambda(x)}{\partial \lambda}dxd\lambda\\
&=\int_0^1 E_{p_\lambda(x)}\left[\frac{dU\lambda(x)}{d\lambda}\right]d\lambda = \int_0^1 E_{p(x)}\left[U_B(x) - U_A(x)\right]d\lambda
\end{aligned}\]
<p>It is all very nice, but very abstract if you are not a physicist&#33; What does it have to do with Bayesian inference?</p>
<p>The Bayes theorem states that the posterior</p>
<p>\(p(x|y) = \frac{p(x,y)}{p(y)}\) ,</p>
<p>where \(x\) is my hypothesis or the parameters of my model, and \(y\) is my observed data&#33; We can actually rewrite as an energy model, as earlier&#33;</p>
\[p(x|y) = \frac{1}{Z}\exp\left(-U(x,y)\right)\]
<p>where the potential energy \(U(x,y) = -\log(p(x,y))\) is the negative log joint and the partition function \(Z=\int_\Omega \exp(-U(x,y))dx\) is the evidence. We have furthermore the connection that the free energy is nothing else than the log evidence.</p>
<p>Now we have a direct connection between energy models and Bayesian problems&#33; Which leads us to the use of TI for Bayesian inference:</p>
<p>The most difficult part of the Bayes theorem comes from the fact that except for simple cases, the posterior \(p(x|y)\) is intractable. Most of the time the joint distribution \(p(x,y)\) is known in closed form, the real issue is then to estimate the evidence \(p(y)\), which is exactly what TI aims at.</p>
<p>The two systems we are going to consider is the prior : \(U_A(x) = -\log p(x)\) and the joint distribution \(U_B(x) = -\log p(x,y) = -\log p(y|x) - \log p(x)\). Now our intermediate state is given by</p>
\[U_\lambda(x) = -\log p(x) + \lambda(-\log p(y|x) - \log p(x) - \log p(x)) = -\log p(x) - \lambda p(y|x)\]
<p>.</p>
<p>The normalized density derived from this potential energy is called a <strong>power posterior</strong> :</p>
\[p_\lambda(x|y) = \frac{p(y|x)^\lambda p(y)}{Z_\lambda}\]
<p>which just a posterior for which we reduced the importance of the likelihood.</p>
<p>Ok so performing the thermodynamic integration we derived earlier gives us :</p>
\[ \log\int p(x,y)dx - \log \underbrace{\int p(x) dx}_{=1} = \log p(y) = \int_0^1 E_{p_\lambda(x)}[\log p(y|x)]d\lambda\]
<p>Now let&#39;s put this to practice for a very simple use case a Gaussian prior with a Gaussian likelihood</p>
\[\begin{aligned}
p(x) = \mathcal{N}(x|10, 1)
p(y|x) = \mathcal{N}(y|x, 1)
\end{aligned}\]
<p>We take \(y = -10\) to make a clear difference between \(U_A\) and \(U_B\)</p>
<p>Of course the posterior can be found analytically but this will help us to evaluate different approaches.</p>
<pre><code class="language-julia">using Plots, Distributions, LaTeXStrings; pyplot&#40;&#41;; default&#40;lw &#61; 2.0, legendfontsize &#61; 15.0, labelfontsize &#61; 15.0&#41;
σ_prior &#61; 1.0; σ_likelihood &#61; 1.0
μ_prior &#61; 10.0
prior &#61; Normal&#40;μ_prior, σ_prior&#41;
likelihood&#40;x&#41; &#61; Normal&#40;x, σ_likelihood&#41;
y &#61; -10.0
p_prior&#40;x&#41; &#61; pdf&#40;prior, x&#41;
p_likelihood&#40;x, y&#41; &#61; pdf&#40;likelihood&#40;x&#41;, y&#41;</code></pre>

<p>Now we have all the parameters in place we can define the exact posterior and power posterior:</p>
<pre><code class="language-julia">σ_posterior&#40;λ&#41; &#61; sqrt&#40;inv&#40;1/σ_prior^2 &#43; λ / σ_likelihood^2&#41;&#41;
μ_posterior&#40;y, λ&#41; &#61; σ_posterior&#40;λ&#41; * &#40;λ * y / σ_likelihood^2 &#43; μ_prior / σ_prior^2&#41;
posterior&#40;y&#41; &#61; Normal&#40;μ_posterior&#40;y, 1.0&#41;, σ_posterior&#40;1.0&#41;&#41;
p_posterior&#40;x, y&#41; &#61; pdf&#40;posterior&#40;y&#41;, x&#41;
power_posterior&#40;y, λ&#41; &#61; Normal&#40;μ_posterior&#40;y, λ&#41;, σ_posterior&#40;λ&#41;&#41;
p_pow_posterior&#40;x, y, λ&#41; &#61; pdf&#40;power_posterior&#40;y, λ&#41;, x&#41;
xgrid &#61; range&#40;-10, 10, length &#61; 400&#41;
plot&#40;xgrid, p_prior, label &#61; &quot;Prior&quot;, xlabel &#61; &quot;x&quot;&#41;
plot&#33;&#40;xgrid, x-&gt;p_likelihood&#40;x, y&#41;,label &#61;  &quot;Likelihood&quot;&#41;
plot&#33;&#40;xgrid, x-&gt;p_posterior&#40;x, y&#41;, label &#61; &quot;Posterior&quot;&#41;</code></pre>
<p> <img src="/assets/blogposts/thermint/code/output/distributions.svg" alt=""> <img src="/assets/blogposts/thermint/code/output/power_posteriors.svg" alt=""></p>
<p>We can also visualize the energies themselves</p>
<pre><code class="language-julia">U_A&#40;x&#41; &#61; -logpdf&#40;prior, x&#41;
U_B&#40;x, y&#41; &#61; -logpdf&#40;likelihood&#40;x&#41;, y&#41; - logpdf&#40;prior, x&#41;
U_λ&#40;x, y, λ&#41; &#61; -logpdf&#40;prior, x&#41; - λ * logpdf&#40;likelihood&#40;x&#41;, y&#41;</code></pre>
<p> <img src="/assets/blogposts/thermint/code/output/energies.svg" alt=""></p>
<p>Now we can start evaluating the integrand for multiple \(\lambda\) :</p>
<pre><code class="language-julia">M &#61; 100
λs &#61; range&#40;0, 1, length&#61; M&#41;
λs &#61; &#40;&#40;1:M&#41;./M&#41;.^5
expec_λ &#61; zeros&#40;length&#40;λs&#41;&#41;
T &#61; 1000
for &#40;i, λ&#41; in enumerate&#40;λs&#41;
    pow_post &#61; power_posterior&#40;y, λ&#41;
    expec_λ&#91;i&#93; &#61; sum&#40;logpdf&#40;likelihood&#40;x&#41;, y&#41; for x in rand&#40;pow_post, T&#41;&#41; / T
end</code></pre>
<p> <img src="/assets/blogposts/thermint/code/output/expec_log.svg" alt=""></p>
<p>And we can now compare the sum with the actual value of \(Z\) :</p>
<pre><code class="language-julia">using Trapz
logpy &#61; logpdf&#40;posterior&#40;y&#41;, y&#41;
TI_logpy &#61; &#91;trapz&#40;λs&#91;1:i&#93;, expec_λ&#91;1:i&#93;&#41; for i in 1:length&#40;λs&#41;&#93;</code></pre>
<p> <img src="/assets/blogposts/thermint/code/output/thermint.svg" alt=""></p>
<p>Now that&#39;s great but how does it compare to other methods? Well we want to compute the integral \(p(y) = \int p(y|x)p(x)dx\). The most intuitive way is to sample from the prior \(p(x)\) to perform a Monte-Carlo integration:</p>
\[\int p(y|x)p(x)dx \approx \frac{1}{N}\sum_{i=1}^N p(y|x_i)\]
<p>where \(x_i \sim p(x)\).</p>
<pre><code class="language-julia">T &#61; 10000
xs &#61; rand&#40;prior, T&#41;
prior_logpy &#61; &#91;log&#40;mean&#40;pdf.&#40;likelihood.&#40;xs&#91;1:i&#93;&#41;, y&#41;&#41;&#41; for i in 1:T&#93;</code></pre>
<p> <img src="/assets/blogposts/thermint/code/output/prior_integration.svg" alt=""></p>
<p>As you can see the result is quite off. The reason is that it happens a lot that the prior and the likelihood have very little overlap. This leads to a huge variance in the estimate of the integral.</p>
<p>Finally there is another approach called the <strong>harmonic mean estimator</strong>. So far we sampled from <em>power posteriors</em>, <em>the prior</em> but not from the posterior&#33; Most Bayesian inference methods aim at getting samples from the posterior so this would look like an appropriate approach. If we perform naive <strong>importance sampling</strong> :</p>
\[\int \frac{p(x,y)}{p(x|y)}p(x|y)dx \approx \frac{1}{N}\frac{p(x_i,y)}{p(x_i|y)} = p(y),\]
<p>this would simply not work. But if we use an unnormalized version of the density \(\tilde{p}(x|y) \propto p(x|y)\) we get :</p>
\[ p(y) = \frac{\int \frac{p(y|x)p(x)}{\tilde{p}(x|y)}p(x|y)dx}{\int \frac{p(x)}{\tilde{p}(x|y)}p(x|y)dx} \]
<p>Now replacing \(\tilde{p}(x|y)\) by \(p(y|x)p(x)\) we get the equation :</p>
\[ p(y) = \frac{1}{E_{p(x|y)}\left[\frac{1}{p(y|x)}\right]}\approx \left[\frac{1}{N}\sum \frac{1}{p(y|x_i)}\right]^{-1} \]
<p>where \(x_i \sim p(x|y)\). Now the issue with this is that even though both side of the fractions are unbiased, the ratio is not&#33; Experiments tend show this leads to a large bias. Let&#39;s have a look for our problem</p>
<pre><code class="language-julia">T &#61; 10000
xs &#61; rand&#40;posterior&#40;y&#41;, T&#41;
posterior_logpy &#61; &#91;-log&#40;mean&#40;inv.&#40;pdf.&#40;likelihood.&#40;xs&#91;1:i&#93;&#41;, y&#41;&#41;&#41;&#41; for i in 1:T&#93;</code></pre>
<p> <img src="/assets/blogposts/thermint/code/output/posterior_integration.svg" alt=""></p>
<div class="page-foot">
  <div class="copyright">
    &copy; Theo Galy-Fajou. Last modified: November 10, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    
        <script src="https://utteranc.es/client.js"
            repo="theogf/bayesiantribulations"
            issue-term="title"
            label="[Comments]"
            theme="github-light"
            crossorigin="anonymous"
            async>
        </script>
    
  </body>
</html>
